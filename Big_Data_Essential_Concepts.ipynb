{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<head>\n",
    "<script type=\"text/javascript\"\n",
    "  src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\">\n",
    "</script>\n",
    "\n",
    "</head>\n",
    "\n",
    "<style>\n",
    "\n",
    "@font-face {\n",
    "    font-family: \"Computer Modern\";\n",
    "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
    "}\n",
    "#notebook_panel { /* main background */\n",
    "    background: #888;\n",
    "    color: #f6f6f6;\n",
    "}\n",
    "#notebook li { /* More space between bullet points */\n",
    "margin-top:0.8em;\n",
    "}\n",
    "div.text_cell_render{\n",
    "    font-family: 'Arvo' sans-serif;\n",
    "    line-height: 130%;\n",
    "    font-size: 135%;\n",
    "    width:1000px;\n",
    "    margin-left:auto;\n",
    "    margin-right:auto;\n",
    "}\n",
    "div.cell.code_cell {  \n",
    "    font-family: 'Arvo' sans-serif;\n",
    "    line-height: 150%;\n",
    "    font-size: 150%;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "<p class=\"gap05\"<p>\n",
    "<h1>Big Data:</h1>\n",
    "<h2>Essential Concepts and Tools</h2>\n",
    "<p class=\"gap05\"<p>\n",
    "<h3>Darrell Aucoin</h3>\n",
    "\n",
    "<h3>Stats Club</h3>\n",
    "\n",
    "<p class=\"gap2\"<p>\n",
    "</center>\n",
    "> In pioneer days they used oxen for heavy pulling, and when one ox couldn’t budge a log, they didn’t try to grow a larger ox. We shouldn’t be trying for bigger computers, but for more systems of computers.  \n",
    "> —Grace Hoppe\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".input_prompt, .output_prompt {\n",
    "    display:none !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Big Data?\n",
    "![alt text](images/4-Vs-of-big-data.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The 4 V's of Big Data\n",
    "__Volume__: The _quantity_ of data. \n",
    "- Usually too big to fit into the memory of a single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Veracity__: The _quality_ of data can vary. The inconsistency of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Variety__: Data often comes in a variety of __formats__ and __sources__ often needing to be combined for a data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Velocity__: The speed of generation of new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Volume\n",
    "- 100 terabytes of data are uploaded to Facebook daily.\n",
    "\n",
    "- 90% of all data ever created was generated in the past 2 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Problem__: Impossible to do data analysis with one computer on data this size.\n",
    "\n",
    "__Solution__: A distributed computing system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![alt text](images/Yahoo-hadoop-cluster_OSCON_2007.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###Veracity\n",
    "- Big Data sets do not have the controls of regular studies  \n",
    "    - Naming inconsistency  \n",
    "    - Inconsistency in signal strength (i.e. Boston's Bump App, Google Flu)   \n",
    "- Cannot simply assume data missing at random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Veracity naming inconsistency: a musician named several different ways in several different files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###Variety\n",
    "- Most of Big Data is unstructured or semi-structured data  \n",
    "    - Doesn't have the guarantees of SQL\n",
    "- Data can be structured, semi-structured, or unstructured  \n",
    "- Often have to combine various datasets from a variety of sources and formats  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###Velocity\n",
    "- Speed that data is created, stored, analyzed to create actionable intelligence  \n",
    "- Every min:  \n",
    "    - 100 hours is uploaded to Youtube  \n",
    "    - 200 million emails are sent  \n",
    "    - 20 million photos are viewed  \n",
    "\n",
    "- Often need to be very agile in creating a data product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tools\n",
    "## Popular Hadoop Projects\n",
    "__Hadoop__: A distributed file system and MapReduce engine YARN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Spark__: An in-memory based alternative to Hadoop's MapReduce which is better for machine learning algorithms. \n",
    "- Spark SQL, MLlib (machine learning), GraphX (graph-parallel computation), and Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Storm__: Distributed tool for processing fast, large streams of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Cassandra__: NoSQL system implemented on Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Hive__: Allows users to create SQL-like queries (HQL) and convert them to MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__HCatalog__: A centralized metadata management and sharing service for Hadoop, allowing a unified view of all data in Hadoop clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Pig__: An easy to learn hadoop-based language that is adept at very deep, very long data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Mahout__: A data mining library using the most popular data mining algorithms using the Map Reduce model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Non-Hadoop Projects  \n",
    "__NoSQL (Not Only SQL)__: A database that is not based storage and retrieval of tabular relations used in relational databases. Some can provide a distributed database.  \n",
    "\n",
    "__Examples__: MongoDB, CouchDB, Accumulo, and some NoSQL databases are implemented on Hadoop: Cassandra, HBase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__SQL__: Can spill to disk allowing datasets to be larger than memory size.  \n",
    "\n",
    "__MADlib__: Machine learning library extension for PostgreSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hadoop Ecosystem\n",
    "![alt text](images/Big-data_tools_last.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- The blue is the necessary components of a Hadoop Ecosystem\n",
    "- Some tools provide several functionalities.\n",
    "    - i.e. Hadoop is a distributed file system with MapReduce engine and scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distributed File System\n",
    "![alt text](images/Big-data_tools-dfs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "__HDFS (Hadoop Distributed File System)__ is a distributed file-system across multiple interconnected computer systems (nodes).   \n",
    "- Data is stored across multiple hard drives.\n",
    "\n",
    "__Lustre__: DFS used by most enterprise High Performance Clusters (HPC). Usually uses a shared networked drive.\n",
    "\n",
    "__Google File System (GFS)__: Google propriety distributed file system.\n",
    "\n",
    "__MapR__: DFS inspired by HDFS but written in C++ instead of Java. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MapReduce\n",
    "![alt text](images/Big-data_tools-MapReduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- MapReduce is the engine that processes data in a Hadoop Ecosystem.\n",
    "- Spark and Tez uses a more flexiable in memory model of MapReduce which is better for Machine Learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scheduler\n",
    "![alt text](images/Big-data_tools-Scheduler.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In order for multiple people to run on the same cluster, a scheduler is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Manipulation\n",
    "![alt text](images/Big-data_tools-Data-Manipulation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "These are the tools to help parse, transform, and combine various datasets.\n",
    "\n",
    "- Hive, Spark SQL, Impala, Cassandra, and HBase all use a SQL-like language to help manipulate data.\n",
    "- Hive can be implemented using the Spark MapReduce Engine (significantly speeding it it's processes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Analysis\n",
    "![alt text](images/Big-data_tools-ML_algorithms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "There are several Machine Learning algorithms already in place for the Hadoop Ecosystem.\n",
    "\n",
    "- Mahout can be implemented on Spark, Tez, and Hadoop\n",
    "- Spark also has GraphX, which uses graphs to perform analytics (PageRank, etc.)\n",
    "\n",
    "There is also specialized tools:\n",
    "\n",
    "__Hadoop Image Processing Interface (HIPI)__: Image processing package helping to determine image similarity.\n",
    "\n",
    "__SpatialHadoop__: Extension to process datasets of spatial data in Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Serialization\n",
    "![alt text](images/Big-data_tools-serialization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Parsing, transforming and combining the data into a useable dataset can be time consuming. Thus, once a suitable amount of work is done to create a useable dataset it is best to save it for future work. \n",
    "\n",
    "Serialization saves the state of the data, allowing it to be recreated at a later date.\n",
    "\n",
    "- JAVA Serialization is the worst of the above and should only be used for legacy reasons\n",
    "\n",
    "__Avro:__ Serialization made for Hadoop.\n",
    "\n",
    "__JSON:__ Java Script Object Notation is a convenient way of describing, serializing, and transferring data.\n",
    "\n",
    "__Protocol Buffers:__ More optimal serialization that requires the precise structure of the data when job is being run. Has less support for programming languages.\n",
    "\n",
    "__Parquet:__ A columnar data storage format, allowing it perform well for structured data with a fair amount of repetition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Transfer\n",
    "![alt text](images/Big-data_tools-Data_transfer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Data transfer of large amounts of data to and from dfs.\n",
    "\n",
    "__Flume, DistCp__: Move files and flat text into Hadoop.\n",
    "\n",
    "__Sqoop__: Move data between Hadoop and SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Streaming\n",
    "![alt text](images/Big-data_tools-Streaming.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "__Streaming__ provides new calculations based on incoming data.\n",
    "\n",
    "__Example__: Netflix 'Trending Now' feature. \n",
    "Possibly with personalized medicine to use medical devices to detect heart attacks before they happen.\n",
    "\n",
    "__Spark Streaming__: Uses a micro-batch model that checks updates every 0.5-10 seconds and updates it's model.\n",
    "\n",
    "__Storm__: Uses either streaming or micro-batch updates to update model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Management and Monitoring\n",
    "![alt text](images/Big-data_tools-management.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "__Node configuration management__: Puppet, Chef. Change operating system parameters and install software.\n",
    "\n",
    "__Resource Tracking__: Monitor the performance of many tools.\n",
    "\n",
    "__Coordination__: Helps synchronize many tools in a single application: Zookeeper.\n",
    "\n",
    "---\n",
    "\n",
    "__Ambari__: Tool to help install, starting, stopping, and reconfiguring Hadoop cluster.\n",
    "\n",
    "__HCatalog__: Central catalog of file formats and locations of data. Data looks like a table-like to user. \n",
    "\n",
    "__Nagios__: Alert failures and problems through a graphical interface and alert administration though email of problems.\n",
    "\n",
    "__Puppet, Chef__: Manager for configuration of a large number of machines.\n",
    "\n",
    "__Zookeeper__: Helps coordination of tools.\n",
    "\n",
    "__Oozie__: Workflow scheduler to start, stop, suspend and restart jobs, controlling the workflow so that no task is performed before it is ready.\n",
    "\n",
    "__Ganglia__: Visualize how systems are being used and keeping track of general health of cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Security, Access Control, and Auditing\n",
    "![alt text](images/Big-data_tools-security.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Hadoop in itself doesn't provide much security. As Hadoop increased in popularity, so has security projects.\n",
    "\n",
    "Kerberos, Sentry, Knox are such projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cloud Computing and Virtualization\n",
    "![alt text](images/Big-data_tools-Cloud.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Sometimes you only need intermediate use of a cluster and creating/maintaining one of your own is prohibitively expensive. \n",
    "\n",
    "Cloud computing and virtualization tools provide easy construction of Hadoop environments with relative ease on cloud computering environments like [AWS](http://aws.amazon.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distribution Platforms\n",
    "\n",
    "Distribution platforms help (for a cost) easy installation and software maintaince of a Hadoop cluster. \n",
    "\n",
    "- Tool versions are checked for compatability, usually meaning that they are not the newest versions\n",
    "\n",
    "Some of these are: [Cloudera](http://www.cloudera.com/content/cloudera/en/home.html), [MapR](https://www.mapr.com/), and [Hortonworks](http://hortonworks.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# MapReduce  \n",
    "__Problem__: Can't use a single computer to process the data (take too long to process data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Solution__: Use a group of interconnected computers (processor, and memory independent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Problem__: Conventional algorithms are not designed around memory independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Solution__: MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Definition.__ _MapReduce_ is a programming paradigm model of using parallel, distributed algorithims to process or generate data sets. MapRedeuce is composed of two main functions:\n",
    "\n",
    "__Map(k,v)__: Filters and sorts data.\n",
    "\n",
    "__Reduce(k,v)__: Aggregates data according to keys (k)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce Phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce is broken down into several steps:\n",
    "\n",
    "1. Record Reader\n",
    "2. Map\n",
    "3. Combiner (Optional)\n",
    "4. Partitioner\n",
    "5. Shuffle and Sort\n",
    "6. Reduce\n",
    "7. Output Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Record Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Record Reader__ splits input into fixed-size pieces for each mapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- The key is positional information (the number of bytes from start of file) and the value is the chunk of data composing a single record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/MapReduceInput.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- In hadoop, each map task's is an input split which is usually simply a HDFS block\n",
    "    - Hadoop tries scheduling map tasks on nodes where that block is stored (data locality)\n",
    "    - If a file is broken mid-record in a block, hadoop requests the additional information from the next block in the series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Map\n",
    "__Map__ _User defined function_ outputing intermediate key-value pairs\n",
    "![alt text](images/MapReduceMapper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "__key__ ($k_{2}$): Later, MapReduce will group and possibly aggregate data according to these keys, choosing the right keys is here is important for a good MapReduce job.\n",
    "\n",
    "__value__ ($v_{2}$): The data to be grouped according to it's keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Combiner (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Combiner__ _UDF_ that aggregates data according to intermediate keys on a mapper node\n",
    "![alt text](images/MapReduceCombiner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- This can usually reduce the amount of data to be sent over the network increasing efficiency  \n",
    "\n",
    "$$\\left.\\begin{array}{r}\n",
    "\\left(\\mbox{\"hello world\"},1\\right)\\\\\n",
    "\\left(\\mbox{\"hello world\"},1\\right)\\\\\n",
    "\\left(\\mbox{\"hello world\"},1\\right)\n",
    "\\end{array}\\right\\} \\overset{\\mbox{combiner}}{\\longrightarrow}\\left(\\mbox{\"hello world\"},3\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Combiner should be written with the idea that it is executed over most but not all map tasks. ie. $$\\left(k_{2},v_{2}\\right)\\mapsto\\left(k_{2},v_{2}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Usually very similar or the same code as the reduce method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Partitioner\n",
    "__Partitioner__ Sends intermediate key-value pairs (k,v) to reducer by   $$\\mbox{Reducer}=\\mbox{hash}\\left(\\mbox{k}\\right)\\pmod{R}$$  \n",
    "![alt text](images/MapReducePartitioner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- will usually result in a roughly balanced load accross the reducers while ensuring that all key-value pairs are grouped by their key on a single reducer. \n",
    "- A balancer system is in place for the cases when the key-values are too unevenly distributed.\n",
    "- In hadoop, the intermediate keys ($k_{2},v_{2}$) are written to the local harddrive and grouped by which reduce they will be sent to and their key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Shuffle and Sort\n",
    "__Shuffle and Sort__ On reducer node, sorts by key to help group equivalent keys\n",
    "![alt text](images/MapReduceSort.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reduce\n",
    "__Reduce__ _User Defined Function_ that aggregates data (v) according to keys (k) to send key-value pairs to output\n",
    "![alt text](images/MapReduceReduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Output Format\n",
    "__Output Format__ Translates final key-value pairs to file format (tab-seperated by default).\n",
    "![alt text](images/MapReduceOutput.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MapReduce Example: Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://xiaochongzhang.me/blog/wp-content/uploads/2013/05/MapReduce_Work_Structure.png)  \n",
    "Image Source: [Xiaochong Zhang's Blog](http://xiaochongzhang.me/blog/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DAG Models\n",
    "A more flexible form of MapReduce is used by Spark using __Directed Acyclic Graphs (DAG)__. \n",
    "![alt text](images/DAG.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For a set of operations:  \n",
    "\n",
    "1. Create a DAG for operations\n",
    "2. Divide DAG into tasks\n",
    "3. Assign tasks to nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce Programming Models\n",
    "- Looking for parameter(s) ($\\theta$) of a model (mean, parameters of regression, etc.)  \n",
    "  \n",
    "  \n",
    "1. __Partition and Model__: \n",
    "    1. Partition data, \n",
    "    2. Apply unbiased estimator, \n",
    "    3. Average results.\n",
    "2. __Sketching / Sufficient Statistics__: \n",
    "    1. Partition data, \n",
    "    2. Reduce dimensionality of data applicable to model (sufficient statistic or sketch), \n",
    "    3. Construct model from sufficient statistic / sketch.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " \n",
    "## Partition and Model\n",
    "![alt text](images/Partition_model.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Notes  \n",
    "\n",
    " $\\overline{\\hat{\\theta}}$ is as efficient as $\\hat{\\theta}$ with the whole dataset when:\n",
    "1. $\\hat{\\theta}\\sim N\\left(\\theta,\\sigma^{2}\\right)$ \n",
    "2. $x$ is IID and \n",
    "3. $x$ is equally partitioned  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can use algorithms already in R, Python to get $\\hat{\\theta}_i$\n",
    "- $\\overline{\\hat{\\theta}}\\sim N\\left(\\theta,\\frac{\\sigma_{\\hat{\\theta}}^{2}}{\\beta}\\right)$ by Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Restrictions\n",
    "- Values must be IID (i.e. not sorted, etc.)\n",
    "- Model must produce an unbiased estimator of $\\theta$, denoted $\\hat{\\theta}$\n",
    "- $\\sigma_{\\hat{\\theta}}^{2}<\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sketching / Sufficient Statistics\n",
    "__Streaming (Online) Algorithm__: An algorithm in which the input is processed item by item. Due to limited memory and processing time, the algorithm produces a summary or “__sketch__” of the data.\n",
    "\n",
    "__Sufficient Statistic__: A statistic with respect to a model and parameter $\\theta$, such that no other statistic from the sample will provide additional information.\n",
    "\n",
    "- Sketching / Sufficient Statistics programming model aims to break down the algorithm into sketches which is then passed off into the next phase of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt text](images/sketch_model.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Notes  \n",
    "\n",
    "- Not all algorithms can be broken down this way.\n",
    "\n",
    "### Restrictions\n",
    "- All sketches must be communicative and associative\n",
    "    - For each item to be processed, the order of operations __must not__ matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Mean  \n",
    "\n",
    "### Partition and Model\n",
    "  \n",
    "$$\\begin{aligned}\\hat{\\theta}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i} &  & \\overline{\\hat{\\theta}}=\\frac{1}{\\beta}\\sum_{i=1}^{\\beta}\\hat{\\theta}_{i}\\end{aligned}\n",
    "$$\n",
    "![alt text](images/Partition_model_example.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sufficient Statistic / Sketch\n",
    "![alt text](images/sketch_model_example.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Mean Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.random import randint, chisquare\n",
    "import numpy as np\n",
    "X = sc.parallelize(randint(0, 101, 200000))\n",
    "X_part = X.repartition(10).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Mean - Partition and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def mean_part(interator):\n",
    "    x = 0\n",
    "    n = 0\n",
    "    for i in interator:\n",
    "        x+=i\n",
    "        n+=1\n",
    "    avg = x / float(n)\n",
    "    return [[avg, n]]\n",
    "model_mean_part = X_part.mapPartitions(mean_part)  \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[50.336245888157897, 19456],\n",
       " [50.215136718750003, 20480],\n",
       " [50.007421874999999, 20480],\n",
       " [50.135214401294498, 19776],\n",
       " [50.141858552631582, 19456],\n",
       " [50.08115748355263, 19456],\n",
       " [50.2578125, 20480],\n",
       " [50.243945312500003, 20480],\n",
       " [49.786543996710527, 19456],\n",
       " [50.072363281249999, 20480]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mean_part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Mean - Partition and Model\n",
    "The partitions are not equally sized, thus we'll use a weighted average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean via Partition and Model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50.128590000000003"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weighted_avg(theta):\n",
    "    total = 0\n",
    "    weighted_avg = 0\n",
    "    for i in xrange(len(theta)):\n",
    "        weighted_avg += theta[i][0] * theta[i][1]\n",
    "        total+= theta[i][1]\n",
    "    theta_bar = weighted_avg / total\n",
    "    return theta_bar\n",
    "print(\"Mean via Partition and Model:\")\n",
    "weighted_avg(model_mean_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Mean - Sufficient Statistics / Sketching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean via Sketching:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50.128590000000003"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sketch_mean = X_part.map(lambda num: (num, 1)) \\\n",
    "    .reduce(lambda x, y: (x[0]+y[0], x[1]+y[1]) ) \n",
    "x_bar_sketch = sketch_mean[0] / float(sketch_mean[1])\n",
    "print(\"Mean via Sketching:\")\n",
    "x_bar_sketch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Variance  \n",
    " $$\\begin{aligned}\\mbox{Sample Var}=\\hat{\\theta} & =\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}\\right)^{2}\\\\\n",
    " & =\\frac{1}{n-1}\\left[\\sum_{i=1}^{n}x_{i}^{2}-n\\times\\left(\\overline{x}\\right)^{2}\\right]\n",
    "\\end{aligned}$$  \n",
    "\n",
    "$$\\begin{aligned}\\mbox{Sufficient Stat}=\\sum_{i=1}^{N}x_{i}^{2}, &  & \\sum_{i=1}^{N}x_{i}, &  & N\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Variance - Partition and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance via Partitioning:\n",
      "851.095985421\n"
     ]
    }
   ],
   "source": [
    "def var_part(interator):\n",
    "    x = 0\n",
    "    x2 = 0\n",
    "    n = 0\n",
    "    for i in interator:\n",
    "        x += i\n",
    "        x2 += i **2\n",
    "        n += 1\n",
    "    avg = x / float(n)\n",
    "    var = (x2 - n * avg ** 2) / (n-1)\n",
    "    return [[var, n]]\n",
    "var_part_model = X_part.mapPartitions(var_part)   \\\n",
    "    .collect()\n",
    "print(\"Variance via Partitioning:\")\n",
    "print(weighted_avg(var_part_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Variance - Sufficient Statistics / Sketching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance via Sketching:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "851.07917000774989"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sketch_var = X_part.map(lambda num: (num, num**2, 1)) \\\n",
    "    .reduce(lambda x, y: (x[0]+y[0], x[1]+y[1], x[2]+y[2]) ) \n",
    "x_bar_4 = sketch_var[0] / float(sketch_var[2])\n",
    "N = sketch_var[2]\n",
    "print(\"Variance via Sketching:\")\n",
    "(sketch_var[1] - N * x_bar_4 ** 2 ) / (N -1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To understand the MapReduce framework, lets solve a familar problem of Linear Regression. For Hadoop/MapReduce to work we MUST figure out how to parallelize our code, in other words how to use the hadoop system to only need to make a subset of our calculations on a subset of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Assumption__: The value of p, the number of explanatory variables is small enough for R to easily handle i.e.  \n",
    "$$n\\gg p$$  \n",
    "\n",
    "We know from linear regression, that our estimate of $\\hat{\\beta}$:  \n",
    "$$X^{T}X\\hat{\\beta}=X^{T}y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\left(X^{T}X\\right)_{p\\times p}$ and $\\left(X^{T}y\\right)_{p\\times1}$ is small enough for R to solve for $\\hat{\\beta}$, thus we only need $X^{T}X,X^{T}y$ to get $\\hat{\\beta}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To break up this calculation we break our matrix $X$ into submatricies $X_{i}$:  \n",
    "$$X=\\begin{bmatrix}X_{1}\\\\\n",
    "X_{2}\\\\\n",
    "X_{3}\\\\\n",
    "\\vdots\\\\\n",
    "X_{n}\n",
    "\\end{bmatrix}\t\ty=\\begin{bmatrix}y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "y_{3}\\\\\n",
    "\\vdots\\\\\n",
    "y_{n}\n",
    "\\end{bmatrix}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$X^{T}X\t=\t\\begin{bmatrix}X_{1}^{T} & X_{2}^{T} & X_{3}^{T} & \\cdots & X_{n}^{T}\\end{bmatrix}\\begin{bmatrix}X_{1}\\\\\n",
    "X_{2}\\\\\n",
    "X_{3}\\\\\n",
    "\\vdots\\\\\n",
    "X_{n}\n",
    "\\end{bmatrix} \\implies$$\n",
    "\n",
    "$$X^{T}X\t=\t\\begin{bmatrix}X_{1}^{T}X_{1}+ & X_{2}^{T}X_{2}+ & X_{3}^{T}X_{3}+ & \\cdots & +X_{n}^{T}X_{n}\\end{bmatrix}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$X^{T}y=\\begin{bmatrix}X_{1}^{T} & X_{2}^{T} & X_{3}^{T} & \\cdots & X_{n}^{T}\\end{bmatrix}\\begin{bmatrix}y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "y_{3}\\\\\n",
    "\\vdots\\\\\n",
    "y_{n}\n",
    "\\end{bmatrix}\\implies$$\n",
    "\n",
    "$$ X^{T}y=\\begin{bmatrix}X_{1}^{T}y_{1}+ & X_{2}^{T}y_{2}+ & X_{3}^{T}y_{3}+ & \\cdots & +X_{n}^{T}y_{n}\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```R\n",
    "Sys.setenv(\"HADOOP_PREFIX\"=\"/usr/local/hadoop/2.6.0\")\n",
    "Sys.setenv(\"HADOOP_CMD\"=\"/usr/local/hadoop/2.6.0/bin/hadoop\")\n",
    "Sys.setenv(\"HADOOP_STREAMING\"=\n",
    "    \"/usr/local/hadoop/2.6.0/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar\")\n",
    "library(rmr2)\n",
    "library(data.table)\n",
    "#Setup variables \n",
    "p = 10 \n",
    "num.obs = 200\n",
    "beta.true = 1:(p+1) \n",
    "X = cbind(rep(1,num.obs), matrix(rnorm(num.obs * p), \n",
    "    ncol = p))\n",
    "y = X %*% beta.true + rnorm(num.obs) \n",
    "X.index = to.dfs(cbind(y, X)) \n",
    "rm(X, y, num.obs, p) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```R\n",
    "map.XtX = function(., Xi) {\n",
    "    Xi = Xi[,-1] #Get rid of y values in Xi\n",
    "    keyval(1, list(t(Xi) %*% Xi)) \n",
    "}\n",
    "map.Xty = function(., Xi) {\n",
    "    yi = Xi[,1] # Retrieve the y values\n",
    "    Xi = Xi[,-1] #Get rid of y values in Xi\n",
    "    keyval(1, list(t(Xi) %*% yi)) \n",
    "}\n",
    "Sum = function(., YY) {\n",
    "    keyval(1, list(Reduce('+', YY))) \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "Sys.setenv(\"HADOOP_PREFIX\"=\"/usr/local/hadoop/2.6.0\")\n",
    "Sys.setenv(\"HADOOP_CMD\"=\"/usr/local/hadoop/2.6.0/bin/hadoop\")\n",
    "Sys.setenv(\"HADOOP_STREAMING\"=\n",
    "    \"/usr/local/hadoop/2.6.0/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar\")\n",
    "library(rmr2)\n",
    "library(data.table)\n",
    "#Setup variables \n",
    "p = 10 \n",
    "num.obs = 200\n",
    "beta.true = 1:(p+1) \n",
    "X = cbind(rep(1,num.obs), matrix(rnorm(num.obs * p), \n",
    "    ncol = p))\n",
    "y = X %*% beta.true + rnorm(num.obs) \n",
    "X.index = to.dfs(cbind(y, X)) \n",
    "rm(X, y, num.obs, p) \n",
    "##########################\n",
    "map.XtX = function(., Xi) {\n",
    "    Xi = Xi[,-1] #Get rid of y values in Xi\n",
    "    keyval(1, list(t(Xi) %*% Xi)) \n",
    "}\n",
    "map.Xty = function(., Xi) {\n",
    "    yi = Xi[,1] # Retrieve the y values\n",
    "    Xi = Xi[,-1] #Get rid of y values in Xi\n",
    "    keyval(1, list(t(Xi) %*% yi)) \n",
    "}\n",
    "Sum = function(., YY) {\n",
    "    keyval(1, list(Reduce('+', YY))) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- The reason we are returning a list in the map function is because otherwise Reduce will only return a some of the elements of the matricies. \n",
    "\n",
    "    - list prevents this by Reduce iterating though the elements of the list (the individual $X_{i}^{T}X_{i}$  matricies) and applying the binary function '+' to each one.\n",
    "\n",
    "    - list is used in the reduce function `Sum` because we will also use this as a combiner function and if we didn't use a list we would have the same problem as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```R\n",
    "XtX = values(from.dfs(\n",
    "    mapreduce(input = X.index,\n",
    "    map = map.XtX,\n",
    "    reduce = Sum,\n",
    "    combine = TRUE)))[[1]]\n",
    "\n",
    "Xty = values(from.dfs(\n",
    "    mapreduce(\n",
    "    input = X.index,\n",
    "    map = map.Xty,\n",
    "    reduce = Sum,\n",
    "    combine = TRUE)))[[1]]\n",
    "beta.hat = solve(XtX, Xty)\n",
    "print(beta.hat)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           [,1]\n",
       " [1,]  1.045835\n",
       " [2,]  1.980511\n",
       " [3,]  2.993829\n",
       " [4,]  4.011599\n",
       " [5,]  5.074755\n",
       " [6,]  6.008534\n",
       " [7,]  6.947164\n",
       " [8,]  8.024570\n",
       " [9,]  9.024757\n",
       "[10,]  9.888609\n",
       "[11,] 10.893023\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "XtX = values(from.dfs(\n",
    "    mapreduce(input = X.index,\n",
    "    map = map.XtX,\n",
    "    reduce = Sum,\n",
    "    combine = TRUE)))[[1]]\n",
    "\n",
    "Xty = values(from.dfs(\n",
    "    mapreduce(\n",
    "    input = X.index,\n",
    "    map = map.Xty,\n",
    "    reduce = Sum,\n",
    "    combine = TRUE)))[[1]]\n",
    "beta.hat = solve(XtX, Xty)\n",
    "print(beta.hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#sudo ipython nbconvert Big_Data_Essential_Concepts.ipynb --to slides --post serve"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
